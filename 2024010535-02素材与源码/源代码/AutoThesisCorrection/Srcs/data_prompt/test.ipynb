{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pynvml\n",
      "  Using cached pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
      "Installing collected packages: pynvml\n",
      "Successfully installed pynvml-11.5.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m8\n",
      "24576.0\n",
      "316.4375\n",
      "23.69097900390625\n"
     ]
    }
   ],
   "source": [
    "# 查看GPU使用情况\n",
    "! pip install pynvml\n",
    "import pynvml\n",
    "import torch\n",
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0) # 0表示显卡标号\n",
    "device_count=torch.cuda.device_count()\n",
    "print(device_count)\n",
    "# 在每一个要查看的地方都要重新定义一个meminfo \n",
    "meminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "print(meminfo.total/1024**2) #总的显存大小\n",
    "print(meminfo.used/1024**2)  #已用显存大小\n",
    "print(meminfo.free/1024**2)  #剩余显存大小\n",
    "# 单位是MB，如果想看G就再除以一个1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/grammar/anaconda3/lib/python3.11/site-packages (0.1.99)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:256\"\n",
    "! pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9496b905b7d485e8633b5cdbf6d5b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "如果你有任何问题或需要帮助,请随时告诉我。我是一个计算机程序,没有任何情感或意识,但我可以提供最准确的信息和帮助解决问题。\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "model = AutoModel.from_pretrained(\"/home/grammar/Paper_Corrector/backend_interact/mymodel\", trust_remote_code=True).to(device).float()\n",
    "torch.cuda.empty_cache()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/grammar/Paper_Corrector/backend_interact/mymodel\", trust_remote_code=True)\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "response, history = model.chat(tokenizer, \"你有什么要对我说的吗\", history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\"addmm_impl_cpu_\" not implemented for 'Half'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response, history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mchat(tokenizer, \u001b[39m\"\u001b[39m\u001b[39m请判断如下句子是否是病句，如果是，请判断错误的类别并且返回修改后的结果。句子： 做科学上的事实判断，不同的人从不同的视角、不同的方式为出发点，终究可以获得一致的结论，达成某种程度的一致的意见。\u001b[39m\u001b[39m\"\u001b[39m, history\u001b[39m=\u001b[39m[])\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(response)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/THUDM/chatglm2-6b/b1502f4f75c71499a3d566b14463edd62620ce9f/modeling_chatglm.py:1028\u001b[0m, in \u001b[0;36mChatGLMForConditionalGeneration.chat\u001b[0;34m(self, tokenizer, query, history, max_length, num_beams, do_sample, top_p, temperature, logits_processor, **kwargs)\u001b[0m\n\u001b[1;32m   1025\u001b[0m gen_kwargs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m: max_length, \u001b[39m\"\u001b[39m\u001b[39mnum_beams\u001b[39m\u001b[39m\"\u001b[39m: num_beams, \u001b[39m\"\u001b[39m\u001b[39mdo_sample\u001b[39m\u001b[39m\"\u001b[39m: do_sample, \u001b[39m\"\u001b[39m\u001b[39mtop_p\u001b[39m\u001b[39m\"\u001b[39m: top_p,\n\u001b[1;32m   1026\u001b[0m               \u001b[39m\"\u001b[39m\u001b[39mtemperature\u001b[39m\u001b[39m\"\u001b[39m: temperature, \u001b[39m\"\u001b[39m\u001b[39mlogits_processor\u001b[39m\u001b[39m\"\u001b[39m: logits_processor, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs}\n\u001b[1;32m   1027\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuild_inputs(tokenizer, query, history\u001b[39m=\u001b[39mhistory)\n\u001b[0;32m-> 1028\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mgen_kwargs)\n\u001b[1;32m   1029\u001b[0m outputs \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mtolist()[\u001b[39m0\u001b[39m][\u001b[39mlen\u001b[39m(inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]):]\n\u001b[1;32m   1030\u001b[0m response \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(outputs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1565\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1558\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1559\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1560\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1561\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1562\u001b[0m     )\n\u001b[1;32m   1564\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1565\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample(\n\u001b[1;32m   1566\u001b[0m         input_ids,\n\u001b[1;32m   1567\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[1;32m   1568\u001b[0m         logits_warper\u001b[39m=\u001b[39mlogits_warper,\n\u001b[1;32m   1569\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[1;32m   1570\u001b[0m         pad_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mpad_token_id,\n\u001b[1;32m   1571\u001b[0m         eos_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39meos_token_id,\n\u001b[1;32m   1572\u001b[0m         output_scores\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39moutput_scores,\n\u001b[1;32m   1573\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mreturn_dict_in_generate,\n\u001b[1;32m   1574\u001b[0m         synced_gpus\u001b[39m=\u001b[39msynced_gpus,\n\u001b[1;32m   1575\u001b[0m         streamer\u001b[39m=\u001b[39mstreamer,\n\u001b[1;32m   1576\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1577\u001b[0m     )\n\u001b[1;32m   1579\u001b[0m \u001b[39melif\u001b[39;00m is_beam_gen_mode:\n\u001b[1;32m   1580\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_return_sequences \u001b[39m>\u001b[39m generation_config\u001b[39m.\u001b[39mnum_beams:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:2612\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2609\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2611\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2612\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[1;32m   2613\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[1;32m   2614\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   2615\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   2616\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   2617\u001b[0m )\n\u001b[1;32m   2619\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2620\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/THUDM/chatglm2-6b/b1502f4f75c71499a3d566b14463edd62620ce9f/modeling_chatglm.py:932\u001b[0m, in \u001b[0;36mChatGLMForConditionalGeneration.forward\u001b[0;34m(self, input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, return_last_logit)\u001b[0m\n\u001b[1;32m    929\u001b[0m use_cache \u001b[39m=\u001b[39m use_cache \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_cache\n\u001b[1;32m    930\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 932\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer(\n\u001b[1;32m    933\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    934\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m    935\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m    936\u001b[0m     past_key_values\u001b[39m=\u001b[39mpast_key_values,\n\u001b[1;32m    937\u001b[0m     inputs_embeds\u001b[39m=\u001b[39minputs_embeds,\n\u001b[1;32m    938\u001b[0m     use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m    939\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    940\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    941\u001b[0m )\n\u001b[1;32m    943\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    944\u001b[0m \u001b[39mif\u001b[39;00m return_last_logit:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/THUDM/chatglm2-6b/b1502f4f75c71499a3d566b14463edd62620ce9f/modeling_chatglm.py:828\u001b[0m, in \u001b[0;36mChatGLMModel.forward\u001b[0;34m(self, input_ids, position_ids, attention_mask, full_attention_mask, past_key_values, inputs_embeds, use_cache, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    825\u001b[0m rotary_pos_emb \u001b[39m=\u001b[39m rotary_pos_emb\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m    827\u001b[0m \u001b[39m# Run encoder.\u001b[39;00m\n\u001b[0;32m--> 828\u001b[0m hidden_states, presents, all_hidden_states, all_self_attentions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[1;32m    829\u001b[0m     inputs_embeds, full_attention_mask, rotary_pos_emb\u001b[39m=\u001b[39mrotary_pos_emb,\n\u001b[1;32m    830\u001b[0m     kv_caches\u001b[39m=\u001b[39mpast_key_values, use_cache\u001b[39m=\u001b[39muse_cache, output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states\n\u001b[1;32m    831\u001b[0m )\n\u001b[1;32m    833\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[1;32m    834\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(v \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m [hidden_states, presents, all_hidden_states, all_self_attentions] \u001b[39mif\u001b[39;00m v \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/THUDM/chatglm2-6b/b1502f4f75c71499a3d566b14463edd62620ce9f/modeling_chatglm.py:638\u001b[0m, in \u001b[0;36mGLMTransformer.forward\u001b[0;34m(self, hidden_states, attention_mask, rotary_pos_emb, kv_caches, use_cache, output_hidden_states)\u001b[0m\n\u001b[1;32m    629\u001b[0m     layer_ret \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    630\u001b[0m         layer,\n\u001b[1;32m    631\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    635\u001b[0m         use_cache\n\u001b[1;32m    636\u001b[0m     )\n\u001b[1;32m    637\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 638\u001b[0m     layer_ret \u001b[39m=\u001b[39m layer(\n\u001b[1;32m    639\u001b[0m         hidden_states,\n\u001b[1;32m    640\u001b[0m         attention_mask,\n\u001b[1;32m    641\u001b[0m         rotary_pos_emb,\n\u001b[1;32m    642\u001b[0m         kv_cache\u001b[39m=\u001b[39mkv_caches[index],\n\u001b[1;32m    643\u001b[0m         use_cache\u001b[39m=\u001b[39muse_cache\n\u001b[1;32m    644\u001b[0m     )\n\u001b[1;32m    645\u001b[0m hidden_states, kv_cache \u001b[39m=\u001b[39m layer_ret\n\u001b[1;32m    646\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/THUDM/chatglm2-6b/b1502f4f75c71499a3d566b14463edd62620ce9f/modeling_chatglm.py:542\u001b[0m, in \u001b[0;36mGLMBlock.forward\u001b[0;34m(self, hidden_states, attention_mask, rotary_pos_emb, kv_cache, use_cache)\u001b[0m\n\u001b[1;32m    540\u001b[0m layernorm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    541\u001b[0m \u001b[39m# Self attention.\u001b[39;00m\n\u001b[0;32m--> 542\u001b[0m attention_output, kv_cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attention(\n\u001b[1;32m    543\u001b[0m     layernorm_output,\n\u001b[1;32m    544\u001b[0m     attention_mask,\n\u001b[1;32m    545\u001b[0m     rotary_pos_emb,\n\u001b[1;32m    546\u001b[0m     kv_cache\u001b[39m=\u001b[39mkv_cache,\n\u001b[1;32m    547\u001b[0m     use_cache\u001b[39m=\u001b[39muse_cache\n\u001b[1;32m    548\u001b[0m )\n\u001b[1;32m    550\u001b[0m \u001b[39m# Residual connection.\u001b[39;00m\n\u001b[1;32m    551\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_residual_connection_post_layernorm:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/THUDM/chatglm2-6b/b1502f4f75c71499a3d566b14463edd62620ce9f/modeling_chatglm.py:374\u001b[0m, in \u001b[0;36mSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, rotary_pos_emb, kv_cache, use_cache)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    362\u001b[0m         \u001b[39mself\u001b[39m, hidden_states, attention_mask, rotary_pos_emb, kv_cache\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, use_cache\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    363\u001b[0m ):\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    372\u001b[0m \n\u001b[1;32m    373\u001b[0m     \u001b[39m# Attention heads [sq, b, h] --> [sq, b, (np * 3 * hn)]\u001b[39;00m\n\u001b[0;32m--> 374\u001b[0m     mixed_x_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery_key_value(hidden_states)\n\u001b[1;32m    376\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmulti_query_attention:\n\u001b[1;32m    377\u001b[0m         (query_layer, key_layer, value_layer) \u001b[39m=\u001b[39m mixed_x_layer\u001b[39m.\u001b[39msplit(\n\u001b[1;32m    378\u001b[0m             [\n\u001b[1;32m    379\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_attention_heads_per_partition \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size_per_attention_head,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    383\u001b[0m             dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m    384\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlinear(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"addmm_impl_cpu_\" not implemented for 'Half'"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \"请判断如下句子是否是病句，如果是，请判断错误的类别并且返回修改后的结果。句子： 做科学上的事实判断，不同的人从不同的视角、不同的方式为出发点，终究可以获得一致的结论，达成某种程度的一致的意见。\", history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**下面这些代码在下面的markdown特殊声明之前都不要跑！** 这里是用CLG生成数据作测试集测试模型性能的部分，使用的评价方式是字符串完全匹配。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "jsonfile=open('resp.json','w',encoding='gb2312')\n",
    "count=1\n",
    "notwritten=[]\n",
    "with open('dataset.csv', 'r',encoding='gb2312') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        text=row[1]\n",
    "        # print(text)\n",
    "        with open('prompt.txt','r',encoding='utf-8')as f:\n",
    "            prompt=f.read()\n",
    "            # print(prompt)\n",
    "            prompt=prompt+f\"\\n#\\n{text}\\n#\"\n",
    "            # print(prompt)\n",
    "            response, history = model.chat(tokenizer, prompt, history=[])\n",
    "            # print(response)\n",
    "            try:\n",
    "                respjson=json.loads(response)\n",
    "                \n",
    "            except:\n",
    "                print('not written!')\n",
    "                notwritten.append(count)\n",
    "                pass\n",
    "            json.dump(respjson,jsonfile)\n",
    "            jsonfile.write(',')\n",
    "            print(count)\n",
    "            count+=1\n",
    "    jsonfile.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('resp.json','r',encoding='utf-8')as f:\n",
    "    data=json.load(f)\n",
    "with open('resp.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('resp.csv','w',encoding='utf-8') as f:\n",
    "    writer=csv.writer(f)\n",
    "    writer.writerow(data[0].keys())\n",
    "    for item in data:\n",
    "        writer.writerow(item.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modify&test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "content=[]\n",
    "with open('resp.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    # print(type(reader))\n",
    "    header = next(reader) # 读取表头\n",
    "    num_fields = len(header) # 字段数目\n",
    "    content.append(header)\n",
    "    for row in reader:\n",
    "        # print(type(row))\n",
    "        if len(row) != num_fields:\n",
    "            print(f\"Error: Expected {num_fields} fields in line {reader.line_num}, saw {len(row)}\")\n",
    "            del row[2]\n",
    "            \n",
    "            print('line ',reader.line_num,'has been modified!')\n",
    "        row[0]=row[0].strip()\n",
    "        row[1]=row[1].strip()\n",
    "        row[2]=row[2].strip()\n",
    "        content.append(row)\n",
    "with open('resp.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transform encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'gb2312' codec can't decode byte 0xe5 in position 6: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m reader \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mreader(f)\n\u001b[1;32m      5\u001b[0m rows\u001b[39m=\u001b[39m[]\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m reader:\n\u001b[1;32m      7\u001b[0m     row[\u001b[39m1\u001b[39m]\u001b[39m=\u001b[39mrow[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m      8\u001b[0m     rows\u001b[39m.\u001b[39mappend(row)\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'gb2312' codec can't decode byte 0xe5 in position 6: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import csv\n",
    "with open('dataset.csv', 'r', encoding='gb2312') as f:\n",
    "    reader = csv.reader(f)\n",
    "    rows=[row for row in reader]\n",
    "\n",
    "with codecs.open('dataset.csv', 'w', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows=[]\n",
    "with open('dataset.csv', 'r') as f:\n",
    "    reader=csv.reader(f)\n",
    "    for row in reader:\n",
    "        row[1]=row[1].strip()\n",
    "        row[2]=row[2].strip()\n",
    "        rows.append(row)\n",
    "with open('dataset.csv', 'w') as f:\n",
    "    writer= csv.writer(f)\n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     正确句子  \\\n",
      "0       做哲学上的价值判断，每个人都是根据自己的标准来进行，判断的标准是因人而异的，甚至同一个人对同...   \n",
      "1       做哲学上的价值判断，每个人都是根据自己的标准来进行，判断的标准是因人而异的，甚至同一个人对同...   \n",
      "2       做哲学上的价值判断，每个人都是根据自己的标准来进行，判断的标准是因人而异的，甚至同一个人对同...   \n",
      "3       做哲学上的价值判断，每个人都是根据自己的标准来进行，判断的标准是因人而异的，甚至同一个人对同...   \n",
      "4       做哲学上的价值判断，每个人都是根据自己的标准来进行，判断的标准是因人而异的，甚至同一个人对同...   \n",
      "...                                                   ...   \n",
      "222609         \"[20]（p.39）但随着研究的不断深入，学者们得出的结论也逐渐趋向比较客观和公允   \n",
      "222610         \"[20]（p.39）但随着研究的不断深入，学者们得出的结论也逐渐趋向比较客观和公允   \n",
      "222611                      \"、\"白身\"，把缺乏锻炼、阅历不深的文人称作\"白面书生\"等   \n",
      "222612                      \"、\"白身\"，把缺乏锻炼、阅历不深的文人称作\"白面书生\"等   \n",
      "222613                      \"、\"白身\"，把缺乏锻炼、阅历不深的文人称作\"白面书生\"等   \n",
      "\n",
      "                                                       病句  语病类型  Unnamed: 3  \\\n",
      "0       做哲学上的价值判断，每个人都是根据自己的标准来进行，判断的标准是因人而异的，甚至同一个人对同...  成分残缺         NaN   \n",
      "1       做哲学上的价值判断，每个人都是根据自己的标准来进行，判断的标准是因人而异的，甚至同一个人对同...  成分残缺         NaN   \n",
      "2       做哲学上的价值判断，每个人都是根据自己的标准来进行，判断的标准是因人而异的，甚至同一个人对同...  成分残缺         NaN   \n",
      "3       做上的价值判断，每个人都是根据自己的标准来进行，判断的标准是因人而异的，甚至同一个人对同一个...  成分残缺         NaN   \n",
      "4       做哲学上的价值判断，每个人都是根据自己的标准来进行，判断的标准是因人而异的，甚至同一个人对同...  成分残缺         NaN   \n",
      "...                                                   ...   ...         ...   \n",
      "222609      \"\"\"[20]（p.39）但随着研究的不断深入，学者们结论的得出也逐渐趋向比较客观和公允\"  语序不当         NaN   \n",
      "222610      \"\"\"[20]（p.39）学者但随着研究的不断深入，们得出的结论也逐渐趋向比较客观和公允\"  语序不当         NaN   \n",
      "222611                \"\"\"、\"\"白身\"\"，把为锻炼、阅历不深的文人称作\"\"白面书生\"\"等\"  成分残缺         NaN   \n",
      "222612               \"\"\"、\"\"白身\"\"，把缺乏锻炼、阅历不深的文人称作\"\"白面书生\"\"或\"  不合逻辑         NaN   \n",
      "222613               \"\"\"、\"\"白身\"\"，把缺乏锻炼、阅历不文人的深称作\"\"白面书生\"\"等\"  语序不当         NaN   \n",
      "\n",
      "       Unnamed: 4 Unnamed: 5  \n",
      "0             NaN        NaN  \n",
      "1             NaN        NaN  \n",
      "2             NaN        NaN  \n",
      "3             NaN        NaN  \n",
      "4             NaN        NaN  \n",
      "...           ...        ...  \n",
      "222609        NaN        NaN  \n",
      "222610        NaN        NaN  \n",
      "222611        NaN        NaN  \n",
      "222612        NaN        NaN  \n",
      "222613        NaN        NaN  \n",
      "\n",
      "[222614 rows x 6 columns]\n",
      "                                                     病句   语病类型  \\\n",
      "0     做哲学上的价值判断，每个人都是根据自己的标准来进行，判断的标准是因人而异的，甚至同一个人对同...   句式杂糅   \n",
      "1     做哲学上的价值判断，每个人都是根据自己的标准来进行，判断的标准是因人而异的，甚至同一个人对同...   句式杂糅   \n",
      "2     做哲学上的价值判断，每个人都是根据自己的标准来进行，判断的标准是因人而异的，甚至同一个人对同...   句式杂糅   \n",
      "3     做上价值判断，每个人都是根据自己的标准来进行，判断的标准是因人而异的，甚至同一个人对同一个问...   句式杂糅   \n",
      "4     做哲学上的价值判断，每个人都是根据自己的标准来进行，判断的标准是因人而异的，甚至同一个人对同...   句式杂糅   \n",
      "...                                                 ...    ...   \n",
      "1024      作为传统语言文化得以传承的重要媒介而言，汉语言文学课程应当得到中职以及教师中职生的全面关注   语序不当   \n",
      "1025  作为传统文化的重要组成部分，茶文化是中华民族的知识共同体，因而能够良好地赢得人们的统一认识，...  表达不恰当   \n",
      "1026  作为传统文化的重要组成部分，茶文化是中华民族的知识共同体，因而能够很地赢得人们的统一认识，并...   语序不当   \n",
      "1027  作为传统文化的重要组成部分，茶文化是中华民族的知识共同体，因而能够很好地赢得的共同认识，并且...   搭配不当   \n",
      "1028  作为传统文化的重要组成部分，茶文化是中华民族的知识共同体，因而能够很好地赢得人们的统一认识，...   搭配不当   \n",
      "\n",
      "                                                   修正结果  \n",
      "0     做哲学上的价值判断，每个人都是根据自己的标准来进行，判断的标准是因人而异的，甚至同一个人对同...  \n",
      "1     做哲学上的价值判断，每个人都是根据自己的标准来进行，判断的标准是因人而异的，甚至同一个人对同...  \n",
      "2     做哲学上的价值判断，每个人都是根据自己的标准来进行，判断的标准是因人而异的，甚至同一个人对同...  \n",
      "3     做上价值判断，每个人都是根据自己的标准来进行，判断的标准是因人而异的，甚至同一个人对同一个问...  \n",
      "4     做哲学上的价值判断，每个人都是根据自己的标准来进行，判断的标准是因人而异的，甚至同一个人对同...  \n",
      "...                                                 ...  \n",
      "1024       作为传统语言文化得以传承的重要媒介，汉语言文学课程应当得到中职以及教师中职生的全面关注。  \n",
      "1025  作为传统文化的重要组成部分，茶文化是中华民族的知识共同体，因而能够良好地赢得人们的统一认识，...  \n",
      "1026  作为传统文化的重要组成部分，茶文化是中华民族的知识共同体，而且能够赢得人们的统一认识，并对人...  \n",
      "1027  作为传统文化的重要组成部分，茶文化是中华民族的共同认识，并且还可对人们的思想启发及升华发挥正...  \n",
      "1028  作为传统文化的重要组成部分，茶文化是中华民族的知识共同体，因而能够很好地赢得人们的统一认识，...  \n",
      "\n",
      "[1029 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df1=pd.read_csv('dataset.csv',low_memory=False)\n",
    "print(df1)\n",
    "# for i in df1:\n",
    "#     print(i)\n",
    "# print(df1.keys)\n",
    "df2=pd.read_csv('resp.csv')\n",
    "print(df2)\n",
    "# for i in df2:\n",
    "    # print(i)\n",
    "df=pd.merge(df2,df1,on=\"病句\",how='left')\n",
    "df.to_csv('test.csv', encoding=\"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "莫名其妙就能跑了。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "judging accuracy: 0.044487427466150864\n",
      "modifying cohesion: 0.0\n"
     ]
    }
   ],
   "source": [
    "with open('test.csv','r')as f:\n",
    "    judge_wrong=0\n",
    "    modify_wrong=0\n",
    "    tot_num=0\n",
    "    reader = csv.reader(f)\n",
    "    header=next(reader)\n",
    "    for row in reader:\n",
    "        if row[2]!=row[5]:\n",
    "            judge_wrong+=1\n",
    "        if row[3]!=row[4]:\n",
    "            modify_wrong+=1\n",
    "        tot_num+=1\n",
    "jug_acc=1-judge_wrong/tot_num\n",
    "mod_acc=1-modify_wrong/tot_num\n",
    "print(\"judging accuracy:\",jug_acc)\n",
    "print(\"modifying cohesion:\",mod_acc)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于代码的特殊性，上面所作测试代码全部保留，之后再次运行的时候只需要执行前面两个单元格就可以了，之后直接执行这下面的代码。接下来要做的是使用NaCGEC的验证集直接对模型做测试，测试的标准是M2-SCORER。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['16岁时我高中退学了，当时我不知我要做什么才好，天天打工赚钱，就去玩儿花钱。',\n",
       " '1940年8月11日，我出生在印度尼西亚西爪哇的万隆市。',\n",
       " '1945年，日本投降时，我和妈妈、哥哥还留在马来西亚的深山。',\n",
       " '1950年10月9日，我来到了这个五花八门的世界。',\n",
       " '1952年3月3日，我出生于印度尼西亚的一个小镇。',\n",
       " '1973年亲属们一知道我母亲生了一个女孩时，邻居们份份的来看父亲，表示自己对他的同情。',\n",
       " '1988年，我复学以后组织了一个学习小组。',\n",
       " '1989、1990、1991年，他获得最优秀推销员赏。',\n",
       " '1990年，回本国来继续研究世界有名的名胜古迹、世界各国的历史，而且对于人类发展史和文明、生活风俗习惯有很厚的知识，为了打下我的经验，1992年到1993年1年间我去世界旅行。',\n",
       " '1992年至今：任印尼邪加达信心旅游公司业务部经理。',\n",
       " '1993年，我考入天津大学，修读美术系，并在1995年毕业，获得美术学位。',\n",
       " '1994年4月17号是我父亲离开我跟我家里人的暗天。',\n",
       " '1994年初发生在中国农村的这件事震惊。妻子得了一种不治之症，十分痛苦，要求丈夫帮助她自杀，使用安乐死来结束生命。',\n",
       " '1994年初，在中国农村，发生了这样一件事：妻子得了一种不治之症，十分痛苦，要求她的丈夫帮助她自杀（即安乐而死）。',\n",
       " '1999年1月，我从大学完学了。',\n",
       " '1999年9月，我进了北京中医药大学。',\n",
       " '2004年春天的有一天，我和女朋友一起逛街的时候，我听到了有一个好听的歌，就是《2002年的第一场雪》，特别好听，我越听越好。',\n",
       " '21世纪的亚洲会遭到香烟的“攻击”。',\n",
       " '如果找不到垃圾桶应保管好，等找到垃圾桶后再丢。',\n",
       " '以下是我本人的简历。姓名：彭莉莉。姓别：女。年龄：二十三岁。未婚。中学：尊孔国民型中学。工人经验：二年。成绩：曾举办过服装展，参加过毕业服装设计展，以“优良”成绩毕业。']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data=[]\n",
    "with open('CGEDdataset/CGED_test.txt','r',encoding='utf-8')as f:\n",
    "    for line in f:\n",
    "        data.append(line.strip())\n",
    "# dataset=pd.DataFrame(data,columns=['Id','original','corrected1','corrected2','corrected3','corrected4'])\n",
    "# dataset.head()\n",
    "data[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2017年《财富》全球论坛自宣布落户广州以来，《财富》杂志将目光聚焦在广州，以财富为媒，向世界展示一个充满底蕴又创新进取的广州。',\n",
       " '它渐渐失去了青苍的颜色，变成嫩黄，变成柔绿，枝条变成细瘦，变成娇弱，好像病了的孩子。',\n",
       " '可是在从原棉制成纱线的过程，就不像穿着光滑舒适的衣服那样让人愉快了：纱厂工人终日面临着音响，尘埃和湿气的。',\n",
       " '热播剧《山海情》的故事发生在宁夏西海固地区。为了保护当地生态环境和群众生活，20世纪八十年代，当地政府组织6万余名西海固人陆续从大山深处搬迁扎根到宁夏平原的闽宁镇，走上了脱贫致富之路。',\n",
       " '老同志不但关心我，就是和他一起入伍的新同志也经常帮助我，这使我非常感动。']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "originals=dataset.original.tolist()\n",
    "originals[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated sentence  1\n",
      "generated sentence  2\n",
      "generated sentence  3\n",
      "generated sentence  4\n",
      "generated sentence  5\n",
      "generated sentence  6\n",
      "generated sentence  7\n",
      "generated sentence  8\n",
      "generated sentence  9\n",
      "generated sentence  10\n",
      "generated sentence  11\n",
      "generated sentence  12\n",
      "generated sentence  13\n",
      "generated sentence  14\n",
      "generated sentence  15\n",
      "generated sentence  16\n",
      "generated sentence  17\n",
      "generated sentence  18\n",
      "generated sentence  19\n",
      "generated sentence  20\n",
      "generated sentence  21\n",
      "generated sentence  22\n",
      "generated sentence  23\n",
      "generated sentence  24\n",
      "generated sentence  25\n",
      "generated sentence  26\n",
      "generated sentence  27\n",
      "generated sentence  28\n",
      "generated sentence  29\n",
      "generated sentence  30\n",
      "generated sentence  31\n",
      "generated sentence  32\n",
      "generated sentence  33\n",
      "generated sentence  34\n",
      "generated sentence  35\n",
      "generated sentence  36\n",
      "generated sentence  37\n",
      "generated sentence  38\n",
      "generated sentence  39\n",
      "generated sentence  40\n",
      "generated sentence  41\n",
      "generated sentence  42\n",
      "generated sentence  43\n",
      "generated sentence  44\n",
      "generated sentence  45\n",
      "generated sentence  46\n",
      "generated sentence  47\n",
      "generated sentence  48\n",
      "generated sentence  49\n",
      "generated sentence  50\n",
      "generated sentence  51\n",
      "generated sentence  52\n",
      "generated sentence  53\n",
      "generated sentence  54\n",
      "generated sentence  55\n",
      "generated sentence  56\n",
      "generated sentence  57\n",
      "generated sentence  58\n",
      "generated sentence  59\n",
      "generated sentence  60\n",
      "generated sentence  61\n",
      "generated sentence  62\n",
      "generated sentence  63\n",
      "generated sentence  64\n",
      "generated sentence  65\n",
      "generated sentence  66\n",
      "generated sentence  67\n",
      "generated sentence  68\n",
      "generated sentence  69\n",
      "generated sentence  70\n",
      "generated sentence  71\n",
      "generated sentence  72\n",
      "generated sentence  73\n",
      "generated sentence  74\n",
      "generated sentence  75\n",
      "generated sentence  76\n",
      "generated sentence  77\n",
      "generated sentence  78\n",
      "generated sentence  79\n",
      "generated sentence  80\n",
      "generated sentence  81\n",
      "generated sentence  82\n",
      "generated sentence  83\n",
      "generated sentence  84\n",
      "generated sentence  85\n",
      "generated sentence  86\n",
      "generated sentence  87\n",
      "generated sentence  88\n",
      "generated sentence  89\n",
      "generated sentence  90\n",
      "generated sentence  91\n",
      "generated sentence  92\n",
      "generated sentence  93\n",
      "generated sentence  94\n",
      "generated sentence  95\n",
      "generated sentence  96\n",
      "generated sentence  97\n",
      "generated sentence  98\n",
      "generated sentence  99\n",
      "generated sentence  100\n",
      "generated sentence  101\n",
      "generated sentence  102\n",
      "generated sentence  103\n",
      "generated sentence  104\n",
      "generated sentence  105\n",
      "generated sentence  106\n",
      "generated sentence  107\n",
      "generated sentence  108\n",
      "generated sentence  109\n",
      "generated sentence  110\n",
      "generated sentence  111\n",
      "generated sentence  112\n",
      "generated sentence  113\n",
      "generated sentence  114\n",
      "generated sentence  115\n",
      "generated sentence  116\n",
      "generated sentence  117\n",
      "generated sentence  118\n",
      "generated sentence  119\n",
      "generated sentence  120\n",
      "generated sentence  121\n",
      "generated sentence  122\n",
      "generated sentence  123\n",
      "generated sentence  124\n",
      "generated sentence  125\n",
      "generated sentence  126\n",
      "generated sentence  127\n",
      "generated sentence  128\n",
      "generated sentence  129\n",
      "generated sentence  130\n",
      "generated sentence  131\n",
      "generated sentence  132\n",
      "generated sentence  133\n",
      "generated sentence  134\n",
      "generated sentence  135\n",
      "generated sentence  136\n",
      "generated sentence  137\n",
      "generated sentence  138\n",
      "generated sentence  139\n",
      "generated sentence  140\n",
      "generated sentence  141\n",
      "generated sentence  142\n",
      "generated sentence  143\n",
      "generated sentence  144\n",
      "generated sentence  145\n",
      "generated sentence  146\n",
      "generated sentence  147\n",
      "generated sentence  148\n",
      "generated sentence  149\n",
      "generated sentence  150\n",
      "generated sentence  151\n",
      "generated sentence  152\n",
      "generated sentence  153\n",
      "generated sentence  154\n",
      "generated sentence  155\n",
      "generated sentence  156\n",
      "generated sentence  157\n",
      "generated sentence  158\n",
      "generated sentence  159\n",
      "generated sentence  160\n",
      "generated sentence  161\n",
      "generated sentence  162\n",
      "generated sentence  163\n",
      "generated sentence  164\n",
      "generated sentence  165\n",
      "generated sentence  166\n",
      "generated sentence  167\n",
      "generated sentence  168\n",
      "generated sentence  169\n",
      "generated sentence  170\n",
      "generated sentence  171\n",
      "generated sentence  172\n",
      "generated sentence  173\n",
      "generated sentence  174\n",
      "generated sentence  175\n",
      "generated sentence  176\n",
      "generated sentence  177\n",
      "generated sentence  178\n",
      "generated sentence  179\n",
      "generated sentence  180\n",
      "generated sentence  181\n",
      "generated sentence  182\n",
      "generated sentence  183\n",
      "generated sentence  184\n",
      "generated sentence  185\n",
      "generated sentence  186\n",
      "generated sentence  187\n",
      "generated sentence  188\n",
      "generated sentence  189\n",
      "generated sentence  190\n",
      "generated sentence  191\n",
      "generated sentence  192\n",
      "generated sentence  193\n",
      "generated sentence  194\n",
      "generated sentence  195\n",
      "generated sentence  196\n",
      "generated sentence  197\n",
      "generated sentence  198\n",
      "generated sentence  199\n",
      "generated sentence  200\n",
      "generated sentence  201\n",
      "generated sentence  202\n",
      "generated sentence  203\n",
      "generated sentence  204\n",
      "generated sentence  205\n",
      "generated sentence  206\n",
      "generated sentence  207\n",
      "generated sentence  208\n",
      "generated sentence  209\n",
      "generated sentence  210\n",
      "generated sentence  211\n",
      "generated sentence  212\n",
      "generated sentence  213\n",
      "generated sentence  214\n",
      "generated sentence  215\n",
      "generated sentence  216\n",
      "generated sentence  217\n",
      "generated sentence  218\n",
      "generated sentence  219\n",
      "generated sentence  220\n",
      "generated sentence  221\n",
      "generated sentence  222\n",
      "generated sentence  223\n",
      "generated sentence  224\n",
      "generated sentence  225\n",
      "generated sentence  226\n",
      "generated sentence  227\n",
      "generated sentence  228\n",
      "generated sentence  229\n",
      "generated sentence  230\n",
      "generated sentence  231\n",
      "generated sentence  232\n",
      "generated sentence  233\n",
      "generated sentence  234\n",
      "generated sentence  235\n",
      "generated sentence  236\n",
      "generated sentence  237\n",
      "generated sentence  238\n",
      "generated sentence  239\n",
      "generated sentence  240\n",
      "generated sentence  241\n",
      "generated sentence  242\n",
      "generated sentence  243\n",
      "generated sentence  244\n",
      "generated sentence  245\n",
      "generated sentence  246\n",
      "generated sentence  247\n",
      "generated sentence  248\n",
      "generated sentence  249\n",
      "generated sentence  250\n",
      "generated sentence  251\n",
      "generated sentence  252\n",
      "generated sentence  253\n",
      "generated sentence  254\n",
      "generated sentence  255\n",
      "generated sentence  256\n",
      "generated sentence  257\n",
      "generated sentence  258\n",
      "generated sentence  259\n",
      "generated sentence  260\n",
      "generated sentence  261\n",
      "generated sentence  262\n",
      "generated sentence  263\n",
      "generated sentence  264\n",
      "generated sentence  265\n",
      "generated sentence  266\n",
      "generated sentence  267\n",
      "generated sentence  268\n",
      "generated sentence  269\n",
      "generated sentence  270\n",
      "generated sentence  271\n",
      "generated sentence  272\n",
      "generated sentence  273\n",
      "generated sentence  274\n",
      "generated sentence  275\n",
      "generated sentence  276\n",
      "generated sentence  277\n",
      "generated sentence  278\n",
      "generated sentence  279\n",
      "generated sentence  280\n",
      "generated sentence  281\n",
      "generated sentence  282\n",
      "generated sentence  283\n",
      "generated sentence  284\n",
      "generated sentence  285\n",
      "generated sentence  286\n",
      "generated sentence  287\n",
      "generated sentence  288\n",
      "generated sentence  289\n",
      "generated sentence  290\n",
      "generated sentence  291\n",
      "generated sentence  292\n",
      "generated sentence  293\n",
      "generated sentence  294\n",
      "generated sentence  295\n",
      "generated sentence  296\n",
      "generated sentence  297\n",
      "generated sentence  298\n",
      "generated sentence  299\n",
      "generated sentence  300\n",
      "generated sentence  301\n",
      "generated sentence  302\n",
      "generated sentence  303\n",
      "generated sentence  304\n",
      "generated sentence  305\n",
      "generated sentence  306\n",
      "generated sentence  307\n",
      "generated sentence  308\n",
      "generated sentence  309\n",
      "generated sentence  310\n",
      "generated sentence  311\n",
      "generated sentence  312\n",
      "generated sentence  313\n",
      "generated sentence  314\n",
      "generated sentence  315\n",
      "generated sentence  316\n",
      "generated sentence  317\n",
      "generated sentence  318\n",
      "generated sentence  319\n",
      "generated sentence  320\n",
      "generated sentence  321\n",
      "generated sentence  322\n",
      "generated sentence  323\n",
      "generated sentence  324\n",
      "generated sentence  325\n",
      "generated sentence  326\n",
      "generated sentence  327\n",
      "generated sentence  328\n",
      "generated sentence  329\n",
      "generated sentence  330\n",
      "generated sentence  331\n",
      "generated sentence  332\n",
      "generated sentence  333\n",
      "generated sentence  334\n",
      "generated sentence  335\n",
      "generated sentence  336\n",
      "generated sentence  337\n",
      "generated sentence  338\n",
      "generated sentence  339\n",
      "generated sentence  340\n",
      "generated sentence  341\n",
      "generated sentence  342\n",
      "generated sentence  343\n",
      "generated sentence  344\n",
      "generated sentence  345\n",
      "generated sentence  346\n",
      "generated sentence  347\n",
      "generated sentence  348\n",
      "generated sentence  349\n",
      "generated sentence  350\n",
      "generated sentence  351\n",
      "generated sentence  352\n",
      "generated sentence  353\n",
      "generated sentence  354\n",
      "generated sentence  355\n",
      "generated sentence  356\n",
      "generated sentence  357\n",
      "generated sentence  358\n",
      "generated sentence  359\n",
      "generated sentence  360\n",
      "generated sentence  361\n",
      "generated sentence  362\n",
      "generated sentence  363\n",
      "generated sentence  364\n",
      "generated sentence  365\n",
      "generated sentence  366\n",
      "generated sentence  367\n",
      "generated sentence  368\n",
      "generated sentence  369\n",
      "generated sentence  370\n",
      "generated sentence  371\n",
      "generated sentence  372\n",
      "generated sentence  373\n",
      "generated sentence  374\n",
      "generated sentence  375\n",
      "generated sentence  376\n",
      "generated sentence  377\n",
      "generated sentence  378\n",
      "generated sentence  379\n",
      "generated sentence  380\n",
      "generated sentence  381\n",
      "generated sentence  382\n",
      "generated sentence  383\n",
      "generated sentence  384\n",
      "generated sentence  385\n",
      "generated sentence  386\n",
      "generated sentence  387\n",
      "generated sentence  388\n",
      "generated sentence  389\n",
      "generated sentence  390\n",
      "generated sentence  391\n",
      "generated sentence  392\n",
      "generated sentence  393\n",
      "generated sentence  394\n",
      "generated sentence  395\n",
      "generated sentence  396\n",
      "generated sentence  397\n",
      "generated sentence  398\n",
      "generated sentence  399\n",
      "generated sentence  400\n",
      "generated sentence  401\n",
      "generated sentence  402\n",
      "generated sentence  403\n",
      "generated sentence  404\n",
      "generated sentence  405\n",
      "generated sentence  406\n",
      "generated sentence  407\n",
      "generated sentence  408\n",
      "generated sentence  409\n",
      "generated sentence  410\n",
      "generated sentence  411\n",
      "generated sentence  412\n",
      "generated sentence  413\n",
      "generated sentence  414\n",
      "generated sentence  415\n",
      "generated sentence  416\n",
      "generated sentence  417\n",
      "generated sentence  418\n",
      "generated sentence  419\n",
      "generated sentence  420\n",
      "generated sentence  421\n",
      "generated sentence  422\n",
      "generated sentence  423\n",
      "generated sentence  424\n",
      "generated sentence  425\n",
      "generated sentence  426\n",
      "generated sentence  427\n",
      "generated sentence  428\n",
      "generated sentence  429\n",
      "generated sentence  430\n",
      "generated sentence  431\n",
      "generated sentence  432\n",
      "generated sentence  433\n",
      "generated sentence  434\n",
      "generated sentence  435\n",
      "generated sentence  436\n",
      "generated sentence  437\n",
      "generated sentence  438\n",
      "generated sentence  439\n",
      "generated sentence  440\n",
      "generated sentence  441\n",
      "generated sentence  442\n",
      "generated sentence  443\n",
      "generated sentence  444\n",
      "generated sentence  445\n",
      "generated sentence  446\n",
      "generated sentence  447\n",
      "generated sentence  448\n",
      "generated sentence  449\n",
      "generated sentence  450\n",
      "generated sentence  451\n",
      "generated sentence  452\n",
      "generated sentence  453\n",
      "generated sentence  454\n",
      "generated sentence  455\n",
      "generated sentence  456\n",
      "generated sentence  457\n",
      "generated sentence  458\n",
      "generated sentence  459\n",
      "generated sentence  460\n",
      "generated sentence  461\n",
      "generated sentence  462\n",
      "generated sentence  463\n",
      "generated sentence  464\n",
      "generated sentence  465\n",
      "generated sentence  466\n",
      "generated sentence  467\n",
      "generated sentence  468\n",
      "generated sentence  469\n",
      "generated sentence  470\n",
      "generated sentence  471\n",
      "generated sentence  472\n",
      "generated sentence  473\n",
      "generated sentence  474\n",
      "generated sentence  475\n",
      "generated sentence  476\n",
      "generated sentence  477\n",
      "generated sentence  478\n",
      "generated sentence  479\n",
      "generated sentence  480\n",
      "generated sentence  481\n",
      "generated sentence  482\n",
      "generated sentence  483\n",
      "generated sentence  484\n",
      "generated sentence  485\n",
      "generated sentence  486\n",
      "generated sentence  487\n",
      "generated sentence  488\n",
      "generated sentence  489\n",
      "generated sentence  490\n",
      "generated sentence  491\n",
      "generated sentence  492\n",
      "generated sentence  493\n",
      "generated sentence  494\n",
      "generated sentence  495\n",
      "generated sentence  496\n",
      "generated sentence  497\n",
      "generated sentence  498\n",
      "generated sentence  499\n",
      "generated sentence  500\n",
      "generated sentence  501\n",
      "generated sentence  502\n",
      "generated sentence  503\n",
      "generated sentence  504\n",
      "generated sentence  505\n",
      "generated sentence  506\n",
      "generated sentence  507\n",
      "generated sentence  508\n",
      "generated sentence  509\n",
      "generated sentence  510\n",
      "generated sentence  511\n",
      "generated sentence  512\n",
      "generated sentence  513\n",
      "generated sentence  514\n",
      "generated sentence  515\n",
      "generated sentence  516\n",
      "generated sentence  517\n",
      "generated sentence  518\n",
      "generated sentence  519\n",
      "generated sentence  520\n",
      "generated sentence  521\n",
      "generated sentence  522\n",
      "generated sentence  523\n",
      "generated sentence  524\n",
      "generated sentence  525\n",
      "generated sentence  526\n",
      "generated sentence  527\n",
      "generated sentence  528\n",
      "generated sentence  529\n",
      "generated sentence  530\n",
      "generated sentence  531\n",
      "generated sentence  532\n",
      "generated sentence  533\n",
      "generated sentence  534\n",
      "generated sentence  535\n",
      "generated sentence  536\n",
      "generated sentence  537\n",
      "generated sentence  538\n",
      "generated sentence  539\n",
      "generated sentence  540\n",
      "generated sentence  541\n",
      "generated sentence  542\n",
      "generated sentence  543\n",
      "generated sentence  544\n",
      "generated sentence  545\n",
      "generated sentence  546\n",
      "generated sentence  547\n",
      "generated sentence  548\n",
      "generated sentence  549\n",
      "generated sentence  550\n",
      "generated sentence  551\n",
      "generated sentence  552\n",
      "generated sentence  553\n",
      "generated sentence  554\n",
      "generated sentence  555\n",
      "generated sentence  556\n",
      "generated sentence  557\n",
      "generated sentence  558\n",
      "generated sentence  559\n",
      "generated sentence  560\n",
      "generated sentence  561\n",
      "generated sentence  562\n",
      "generated sentence  563\n",
      "generated sentence  564\n",
      "generated sentence  565\n",
      "generated sentence  566\n",
      "generated sentence  567\n",
      "generated sentence  568\n",
      "generated sentence  569\n",
      "generated sentence  570\n",
      "generated sentence  571\n",
      "generated sentence  572\n",
      "generated sentence  573\n",
      "generated sentence  574\n",
      "generated sentence  575\n",
      "generated sentence  576\n",
      "generated sentence  577\n",
      "generated sentence  578\n",
      "generated sentence  579\n",
      "generated sentence  580\n",
      "generated sentence  581\n",
      "generated sentence  582\n",
      "generated sentence  583\n",
      "generated sentence  584\n",
      "generated sentence  585\n",
      "generated sentence  586\n",
      "generated sentence  587\n",
      "generated sentence  588\n",
      "generated sentence  589\n",
      "generated sentence  590\n",
      "generated sentence  591\n",
      "generated sentence  592\n",
      "generated sentence  593\n",
      "generated sentence  594\n",
      "generated sentence  595\n",
      "generated sentence  596\n",
      "generated sentence  597\n",
      "generated sentence  598\n",
      "generated sentence  599\n",
      "generated sentence  600\n",
      "generated sentence  601\n",
      "generated sentence  602\n",
      "generated sentence  603\n",
      "generated sentence  604\n",
      "generated sentence  605\n",
      "generated sentence  606\n",
      "generated sentence  607\n",
      "generated sentence  608\n",
      "generated sentence  609\n",
      "generated sentence  610\n",
      "generated sentence  611\n",
      "generated sentence  612\n",
      "generated sentence  613\n",
      "generated sentence  614\n",
      "generated sentence  615\n",
      "generated sentence  616\n",
      "generated sentence  617\n",
      "generated sentence  618\n",
      "generated sentence  619\n",
      "generated sentence  620\n",
      "generated sentence  621\n",
      "generated sentence  622\n",
      "generated sentence  623\n",
      "generated sentence  624\n",
      "generated sentence  625\n",
      "generated sentence  626\n",
      "generated sentence  627\n",
      "generated sentence  628\n",
      "generated sentence  629\n",
      "generated sentence  630\n",
      "generated sentence  631\n",
      "generated sentence  632\n",
      "generated sentence  633\n",
      "generated sentence  634\n",
      "generated sentence  635\n",
      "generated sentence  636\n",
      "generated sentence  637\n",
      "generated sentence  638\n",
      "generated sentence  639\n",
      "generated sentence  640\n",
      "generated sentence  641\n",
      "generated sentence  642\n",
      "generated sentence  643\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m prompt\u001b[39m=\u001b[39mprompt\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m```\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39morigin\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m```\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[39m# print(prompt)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m response, history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mchat(tokenizer, prompt, history\u001b[39m=\u001b[39m[],temperature\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mgenerated sentence \u001b[39m\u001b[39m\"\u001b[39m,count)\n\u001b[1;32m     19\u001b[0m \u001b[39m# print(response)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/THUDM/chatglm2-6b/b1502f4f75c71499a3d566b14463edd62620ce9f/modeling_chatglm.py:1028\u001b[0m, in \u001b[0;36mChatGLMForConditionalGeneration.chat\u001b[0;34m(self, tokenizer, query, history, max_length, num_beams, do_sample, top_p, temperature, logits_processor, **kwargs)\u001b[0m\n\u001b[1;32m   1025\u001b[0m gen_kwargs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m: max_length, \u001b[39m\"\u001b[39m\u001b[39mnum_beams\u001b[39m\u001b[39m\"\u001b[39m: num_beams, \u001b[39m\"\u001b[39m\u001b[39mdo_sample\u001b[39m\u001b[39m\"\u001b[39m: do_sample, \u001b[39m\"\u001b[39m\u001b[39mtop_p\u001b[39m\u001b[39m\"\u001b[39m: top_p,\n\u001b[1;32m   1026\u001b[0m               \u001b[39m\"\u001b[39m\u001b[39mtemperature\u001b[39m\u001b[39m\"\u001b[39m: temperature, \u001b[39m\"\u001b[39m\u001b[39mlogits_processor\u001b[39m\u001b[39m\"\u001b[39m: logits_processor, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs}\n\u001b[1;32m   1027\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuild_inputs(tokenizer, query, history\u001b[39m=\u001b[39mhistory)\n\u001b[0;32m-> 1028\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mgen_kwargs)\n\u001b[1;32m   1029\u001b[0m outputs \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mtolist()[\u001b[39m0\u001b[39m][\u001b[39mlen\u001b[39m(inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]):]\n\u001b[1;32m   1030\u001b[0m response \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(outputs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1565\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1558\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1559\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1560\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1561\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1562\u001b[0m     )\n\u001b[1;32m   1564\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1565\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample(\n\u001b[1;32m   1566\u001b[0m         input_ids,\n\u001b[1;32m   1567\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[1;32m   1568\u001b[0m         logits_warper\u001b[39m=\u001b[39mlogits_warper,\n\u001b[1;32m   1569\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[1;32m   1570\u001b[0m         pad_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mpad_token_id,\n\u001b[1;32m   1571\u001b[0m         eos_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39meos_token_id,\n\u001b[1;32m   1572\u001b[0m         output_scores\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39moutput_scores,\n\u001b[1;32m   1573\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mreturn_dict_in_generate,\n\u001b[1;32m   1574\u001b[0m         synced_gpus\u001b[39m=\u001b[39msynced_gpus,\n\u001b[1;32m   1575\u001b[0m         streamer\u001b[39m=\u001b[39mstreamer,\n\u001b[1;32m   1576\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1577\u001b[0m     )\n\u001b[1;32m   1579\u001b[0m \u001b[39melif\u001b[39;00m is_beam_gen_mode:\n\u001b[1;32m   1580\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_return_sequences \u001b[39m>\u001b[39m generation_config\u001b[39m.\u001b[39mnum_beams:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:2612\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2609\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2611\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2612\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[1;32m   2613\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[1;32m   2614\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   2615\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   2616\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   2617\u001b[0m )\n\u001b[1;32m   2619\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2620\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/THUDM/chatglm2-6b/b1502f4f75c71499a3d566b14463edd62620ce9f/modeling_chatglm.py:932\u001b[0m, in \u001b[0;36mChatGLMForConditionalGeneration.forward\u001b[0;34m(self, input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, return_last_logit)\u001b[0m\n\u001b[1;32m    929\u001b[0m use_cache \u001b[39m=\u001b[39m use_cache \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_cache\n\u001b[1;32m    930\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 932\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer(\n\u001b[1;32m    933\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    934\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m    935\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m    936\u001b[0m     past_key_values\u001b[39m=\u001b[39mpast_key_values,\n\u001b[1;32m    937\u001b[0m     inputs_embeds\u001b[39m=\u001b[39minputs_embeds,\n\u001b[1;32m    938\u001b[0m     use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m    939\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    940\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    941\u001b[0m )\n\u001b[1;32m    943\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    944\u001b[0m \u001b[39mif\u001b[39;00m return_last_logit:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/THUDM/chatglm2-6b/b1502f4f75c71499a3d566b14463edd62620ce9f/modeling_chatglm.py:828\u001b[0m, in \u001b[0;36mChatGLMModel.forward\u001b[0;34m(self, input_ids, position_ids, attention_mask, full_attention_mask, past_key_values, inputs_embeds, use_cache, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    825\u001b[0m rotary_pos_emb \u001b[39m=\u001b[39m rotary_pos_emb\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m    827\u001b[0m \u001b[39m# Run encoder.\u001b[39;00m\n\u001b[0;32m--> 828\u001b[0m hidden_states, presents, all_hidden_states, all_self_attentions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[1;32m    829\u001b[0m     inputs_embeds, full_attention_mask, rotary_pos_emb\u001b[39m=\u001b[39mrotary_pos_emb,\n\u001b[1;32m    830\u001b[0m     kv_caches\u001b[39m=\u001b[39mpast_key_values, use_cache\u001b[39m=\u001b[39muse_cache, output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states\n\u001b[1;32m    831\u001b[0m )\n\u001b[1;32m    833\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[1;32m    834\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(v \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m [hidden_states, presents, all_hidden_states, all_self_attentions] \u001b[39mif\u001b[39;00m v \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/THUDM/chatglm2-6b/b1502f4f75c71499a3d566b14463edd62620ce9f/modeling_chatglm.py:638\u001b[0m, in \u001b[0;36mGLMTransformer.forward\u001b[0;34m(self, hidden_states, attention_mask, rotary_pos_emb, kv_caches, use_cache, output_hidden_states)\u001b[0m\n\u001b[1;32m    629\u001b[0m     layer_ret \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    630\u001b[0m         layer,\n\u001b[1;32m    631\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    635\u001b[0m         use_cache\n\u001b[1;32m    636\u001b[0m     )\n\u001b[1;32m    637\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 638\u001b[0m     layer_ret \u001b[39m=\u001b[39m layer(\n\u001b[1;32m    639\u001b[0m         hidden_states,\n\u001b[1;32m    640\u001b[0m         attention_mask,\n\u001b[1;32m    641\u001b[0m         rotary_pos_emb,\n\u001b[1;32m    642\u001b[0m         kv_cache\u001b[39m=\u001b[39mkv_caches[index],\n\u001b[1;32m    643\u001b[0m         use_cache\u001b[39m=\u001b[39muse_cache\n\u001b[1;32m    644\u001b[0m     )\n\u001b[1;32m    645\u001b[0m hidden_states, kv_cache \u001b[39m=\u001b[39m layer_ret\n\u001b[1;32m    646\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/THUDM/chatglm2-6b/b1502f4f75c71499a3d566b14463edd62620ce9f/modeling_chatglm.py:563\u001b[0m, in \u001b[0;36mGLMBlock.forward\u001b[0;34m(self, hidden_states, attention_mask, rotary_pos_emb, kv_cache, use_cache)\u001b[0m\n\u001b[1;32m    560\u001b[0m layernorm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_attention_layernorm(layernorm_input)\n\u001b[1;32m    562\u001b[0m \u001b[39m# MLP.\u001b[39;00m\n\u001b[0;32m--> 563\u001b[0m mlp_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(layernorm_output)\n\u001b[1;32m    565\u001b[0m \u001b[39m# Second residual connection.\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_residual_connection_post_layernorm:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/THUDM/chatglm2-6b/b1502f4f75c71499a3d566b14463edd62620ce9f/modeling_chatglm.py:499\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    497\u001b[0m intermediate_parallel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation_func(intermediate_parallel)\n\u001b[1;32m    498\u001b[0m \u001b[39m# [s, b, h]\u001b[39;00m\n\u001b[0;32m--> 499\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense_4h_to_h(intermediate_parallel)\n\u001b[1;32m    500\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlinear(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# print(corrections)\n",
    "# print(prompt)\n",
    "corrections=[]\n",
    "count=1\n",
    "for origin in data:\n",
    "    prompt=\"\"\"你的任务是基于接下来提供的\"原始病句\"给出修改后的正确句子。\"原始病句\"将会以三个反引号包裹起来给出。\\\n",
    "病句需要修改的地方只会出现以下语病中的一种或几种：Structural Confusion,Improper Logicality,Missing Component,Redundant Component,Improper Collocation,Improper Word Order.\\n\n",
    "你的回复遵循下面这种格式:\\n\n",
    "{\n",
    "    'origin':'...',\n",
    "    'corrected':'...'\n",
    "}\n",
    "其中'origin'后面照抄接下来提供的病句,'corrected'后面放你的改句，如果你认为没有语病，则复制\"原始病句\".\n",
    "\\n原始病句：\"\"\"\n",
    "    prompt=prompt+\"```\"+origin+\"```\"\n",
    "    # print(prompt)\n",
    "    response, history = model.chat(tokenizer, prompt, history=[],temperature=0.01)\n",
    "    print(\"generated sentence \",count)\n",
    "    # print(response)\n",
    "    count+=1\n",
    "    corrections.append(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrections \n",
    "with open('CGED_sys.txt','w',encoding='utf-8')as f:\n",
    "    for co in corrections:\n",
    "        f.write(co)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corrections' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m corrections[\u001b[39m0\u001b[39m]\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m    \u001b[39m\u001b[39m'\u001b[39m\u001b[39morigin\u001b[39m\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m'\u001b[39m\u001b[39m...\u001b[39m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m    \u001b[39m\u001b[39m'\u001b[39m\u001b[39mcorrected\u001b[39m\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m'\u001b[39m\u001b[39m2017年《财富》全球论坛落户广州以来，《财富》杂志将目光聚焦在广州，以财富为媒，向世界展示一个充满底蕴又充满活力的广州。\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m}\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m grammar\u001b[39m=\u001b[39mre\u001b[39m.\u001b[39mcompile(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcorrected\u001b[39m\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m'\u001b[39m\u001b[39m(?P<content>.*?)\u001b[39m\u001b[39m'\u001b[39m\u001b[39m|改进后的句子:```(?P<content>.*?)`\u001b[39m\u001b[39m\"\u001b[39m,re\u001b[39m.\u001b[39mS)\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39mlen\u001b[39m(corrections)):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corrections' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "corrections[0]=\"{\\n    'origin':'...',\\n    'corrected':'2017年《财富》全球论坛落户广州以来，《财富》杂志将目光聚焦在广州，以财富为媒，向世界展示一个充满底蕴又充满活力的广州。'\\n}\"\n",
    "grammar=re.compile(r\"'corrected':'(?P<content>.*?)'|改进后的句子:```(?P<content>.*?)`\",re.S)\n",
    "for i in range(0,len(corrections)):\n",
    "    correction=re.search(grammar,corrections[i])\n",
    "    print(correction.group('content'))\n",
    "    # break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:256\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97a2aa858234106b29a75b44b8b69eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "作为一名人工智能语言模型,我没有实际的情感或意图,也没有自己的思想或意见。我只是根据我所接受到的问题和指令,提供尽可能准确和有用的答案。如果你有任何需要帮助的问题或需要信息,请随时问我,我会尽力回答。\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "model = AutoModel.from_pretrained(\"\", trust_remote_code=True).to(device).float()\n",
    "torch.cuda.empty_cache()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True)\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "response, history = model.chat(tokenizer, \"你有什么要对我说的吗\", history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"中文语法纠错任务：将一段中文句子进行修正，如果句子没有明显语病则返回原句。\n",
    "\n",
    "例如:\n",
    "\n",
    "学生大概做飞机去北京。 -> 学生大概坐飞机去北京。\\n\n",
    "吸烟者反对这样的措施。 -> 吸烟者反对这样的措施。\\n\n",
    "\n",
    "请对下述句子进行修正。返回你修改后的句子，无需其它说明和解释。\n",
    "\n",
    "xxxxxx ->\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def get_prompt(text):\n",
    "    return prompt.replace('xxxxxx',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文语法纠错任务：将一段中文句子进行修正，如果句子没有明显语病则返回原句。\n",
      "\n",
      "下面是一些范例:\n",
      "\n",
      "学生大概做飞机去北京。 -> 学生大概坐飞机去北京。\n",
      "\n",
      "我觉得他很大胆，他不去外国。他觉得如果要改转变中国，所以就要从中国做的研究。  -> 我觉得他很大胆，他不去外国。他觉得如果要改变中国，就要在中国做研究。\n",
      "\n",
      "吸烟者反对这样的措施。 -> 吸烟者反对这样的措施。\n",
      "\n",
      "\n",
      "请对下述句子进行修正。返回你修改后的句子，无需其它说明和解释。\n",
      "\n",
      "我觉得喜欢一种流行歌曲是盲目的追求一种东西一样。 ->\n",
      "\n",
      "\n",
      "我觉得喜欢一种流行歌曲是盲目地追求一种东西。\n"
     ]
    }
   ],
   "source": [
    "print(get_prompt('我觉得喜欢一种流行歌曲是盲目的追求一种东西一样。'))\n",
    "response, his = model.chat(tokenizer, get_prompt('我觉得喜欢一种流行歌曲是盲目的追求一种东西一样。'), history=[])\n",
    "print(response)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#增加4个范例\n",
    "his.append((\"因为污染也很高，我们要回收垃圾。为什么?塑料、电池、垃圾是我们做的东西。 -> \",\"因为污染很严重，所以我们要回收垃圾。为什么?因为塑料、电池等垃圾是我们人类造出来的东西。\"))\n",
    "his.append((\"最近吸烟者率是越来越多，而且吸烟者的年龄层是越来越少。这些问题是现在最重要的事。 -> \",\"最近吸烟率是越来越高，而且吸烟者的年龄层是越来越低。这些问题是现在最重要的事。\"))\n",
    "\n",
    "his.append((\"不仅他学习某种习惯，他们性格也开始养成。 -> \",\"他们不仅开始学习某种习惯，他们的性格也开始形成。\"))\n",
    "his.append((\"这世界上是很残酷的。 -> \",\"这世界是很残酷的。\"))\n",
    "his.append((\"每一刻都用得无可挑剔。 -> \",\"每一刻都用得无可挑剔。\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我认为控制青少年的吸烟最重要的是家庭教育。\n",
      "但是吸烟者却不考虑被吸烟者的权利和健康。政府应该保护被吸烟者的权利和健康。\n",
      "现在,我的生活非常充实和满意。\n"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \"我认为控制青少年的吸烟最重要的是还是家庭教育。 -> \", history=his)\n",
    "print(response) \n",
    "\n",
    "response, history = model.chat(tokenizer, \"但是吸烟者还不考虑到被吸烟者。应该政府保护被吸烟者的权利和健康。 -> \", history=his)\n",
    "print(response) \n",
    "\n",
    "response, history = model.chat(tokenizer, \"现在，我的生活非常充实，非常满意。 -> \", history=his)\n",
    "print(response) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**必须要经过这样对模型记忆的调整之后，模型才能稳定地返回答案。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'现在,我的生活非常充实和满意。'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#封装成一个函数\n",
    "def predict(text):\n",
    "    response, history = model.chat(tokenizer, f\"{text} ->\", history=his,\n",
    "    temperature=0.01)\n",
    "    return response \n",
    "\n",
    "predict(\"现在，我的生活非常充实，非常满意。\") #可以看到这里的修改就出现错误了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实用小技巧：编码格式不对：在合适的格式下打开文件，剪切；切换文件编码格式并清空内容，粘贴。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_593456/3949351592.py:6: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"finetune/finetunedataset.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>原句</th>\n",
       "      <th>改句</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>阿阿拉三世和斯特拉托妮可之子。</td>\n",
       "      <td>阿里阿拉特三世和斯特拉托妮可之子。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>阿巴杜-马汀的父亲是穆斯林，而母亲则昧基督徒。</td>\n",
       "      <td>阿巴杜-马汀的父亲是穆斯林，而母亲则是基督徒。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>阿巴合在工作中发现，随着村子逐渐赋予，村干部和村民反而有了隔阂。</td>\n",
       "      <td>阿巴合在工作中发现，随着村子逐渐富裕，村干部和村民反而有了隔阂。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>阿巴斯·亚罗斯塔米也是摄影家与诗人，他的摄影作品（1978年至2003年）大部分是雪景，以在...</td>\n",
       "      <td>阿巴斯·基亚罗斯塔米也是摄影家与诗人，他的摄影作品（1978年至2003年）大部分是雪景，以...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>阿巴斯下令巴这勒斯坦警察今后停止止这样的攻击。</td>\n",
       "      <td>阿巴斯下令巴勒斯坦警察今后停止这样的攻击。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  原句  \\\n",
       "0                                    阿阿拉三世和斯特拉托妮可之子。   \n",
       "1                            阿巴杜-马汀的父亲是穆斯林，而母亲则昧基督徒。   \n",
       "2                   阿巴合在工作中发现，随着村子逐渐赋予，村干部和村民反而有了隔阂。   \n",
       "3  阿巴斯·亚罗斯塔米也是摄影家与诗人，他的摄影作品（1978年至2003年）大部分是雪景，以在...   \n",
       "4                            阿巴斯下令巴这勒斯坦警察今后停止止这样的攻击。   \n",
       "\n",
       "                                                  改句  \n",
       "0                                  阿里阿拉特三世和斯特拉托妮可之子。  \n",
       "1                            阿巴杜-马汀的父亲是穆斯林，而母亲则是基督徒。  \n",
       "2                   阿巴合在工作中发现，随着村子逐渐富裕，村干部和村民反而有了隔阂。  \n",
       "3  阿巴斯·基亚罗斯塔米也是摄影家与诗人，他的摄影作品（1978年至2003年）大部分是雪景，以...  \n",
       "4                              阿巴斯下令巴勒斯坦警察今后停止这样的攻击。  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import datasets \n",
    "\n",
    "\n",
    "df = pd.read_csv(\"finetune/finetunedataset.csv\")\n",
    "df=df.filter(regex=\"[原句改句]\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dic = datasets.Dataset.from_pandas(df).train_test_split(\n",
    "    test_size = 2000,shuffle=True, seed = 43)\n",
    "dftrain = ds_dic['train'].to_pandas()\n",
    "dftest = ds_dic['test'].to_pandas()\n",
    "dftrain.to_parquet('finetune/dftrain.parquet')\n",
    "dftest.to_parquet('finetune/dftest.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考官方构建prompt的方法来构建Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round 1]\n",
      "\n",
      "问：中文语法纠错任务：将一段中文句子进行修正，如果句子没有明显语病则返回原句。\n",
      "\n",
      "下面是一些范例:\n",
      "\n",
      "学生大概做飞机去北京。 -> 学生大概坐飞机去北京。\n",
      "\n",
      "我觉得他很大胆，他不去外国。他觉得如果要改转变中国，所以就要从中国做的研究。  -> 我觉得他很大胆，他不去外国。他觉得如果要改变中国，就要在中国做研究。\n",
      "\n",
      "吸烟者反对这样的措施。 -> 吸烟者反对这样的措施。\n",
      "\n",
      "\n",
      "请对下述句子进行修正。返回你修改后的句子，无需其它说明和解释。\n",
      "\n",
      "我觉得喜欢一种流行歌曲是盲目的追求一种东西一样。 ->\n",
      "\n",
      "\n",
      "\n",
      "答：我觉得喜欢一种流行歌曲是盲目地追求一种东西。\n",
      "\n",
      "[Round 2]\n",
      "\n",
      "问：因为污染也很高，我们要回收垃圾。为什么?塑料、电池、垃圾是我们做的东西。 -> \n",
      "\n",
      "答：因为污染很严重，所以我们要回收垃圾。为什么?因为塑料、电池等垃圾是我们人类造出来的东西。\n",
      "\n",
      "[Round 3]\n",
      "\n",
      "问：最近吸烟者率是越来越多，而且吸烟者的年龄层是越来越少。这些问题是现在最重要的事。 -> \n",
      "\n",
      "答：最近吸烟率是越来越高，而且吸烟者的年龄层是越来越低。这些问题是现在最重要的事。\n",
      "\n",
      "[Round 4]\n",
      "\n",
      "问：不仅他学习某种习惯，他们性格也开始养成。 -> \n",
      "\n",
      "答：他们不仅开始学习某种习惯，他们的性格也开始形成。\n",
      "\n",
      "[Round 5]\n",
      "\n",
      "问：这世界上是很残酷的。 -> \n",
      "\n",
      "答：这世界是很残酷的。\n",
      "\n",
      "[Round 6]\n",
      "\n",
      "问：每一刻都用得无可挑剔。 -> \n",
      "\n",
      "答：每一刻都用得无可挑剔。\n",
      "\n",
      "[Round 7]\n",
      "\n",
      "问：味道不太行 -> \n",
      "\n",
      "答：\n"
     ]
    }
   ],
   "source": [
    "def build_inputs(query, history):\n",
    "    prompt = \"\"\n",
    "    for i, (old_query, response) in enumerate(history):\n",
    "        prompt += \"[Round {}]\\n\\n问：{}\\n\\n答：{}\\n\\n\".format(i + 1, old_query, response)\n",
    "    prompt += \"[Round {}]\\n\\n问：{} -> \\n\\n答：\".format(len(history) + 1, query)\n",
    "    return prompt \n",
    "print(build_inputs('味道不太行',history=his))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据模型设置标准化的问答数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Round 1]\\n\\n问：中文语法纠错任务：将一段中文句子进行修正，如果句子没有明显语病...</td>\n",
       "      <td>按理来说，这种关心应该随着生活水平的提高不断增加才对，现在通过这些单位的执行，反而变成了极大...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Round 1]\\n\\n问：中文语法纠错任务：将一段中文句子进行修正，如果句子没有明显语病...</td>\n",
       "      <td>这些学生往往都具有良好的品质和较高的学习能力，可以通过向优秀榜样学习来提升自我。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Round 1]\\n\\n问：中文语法纠错任务：将一段中文句子进行修正，如果句子没有明显语病...</td>\n",
       "      <td>”【2】士人们的人生观与哲学观自然也随之转变，而其对山水自然美的认识也从以玄对山水转向以佛对...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Round 1]\\n\\n问：中文语法纠错任务：将一段中文句子进行修正，如果句子没有明显语病...</td>\n",
       "      <td>战略成本会计的切入点是尽可能地降低产品成本，加强企业成本优化管理与控制，其关注点有质量控制、...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Round 1]\\n\\n问：中文语法纠错任务：将一段中文句子进行修正，如果句子没有明显语病...</td>\n",
       "      <td>此外，众多高校在财务管理的内部管理制度方面存在较多纰漏，很多规范过于笼统没有实际可行性，从而...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  [Round 1]\\n\\n问：中文语法纠错任务：将一段中文句子进行修正，如果句子没有明显语病...   \n",
       "1  [Round 1]\\n\\n问：中文语法纠错任务：将一段中文句子进行修正，如果句子没有明显语病...   \n",
       "2  [Round 1]\\n\\n问：中文语法纠错任务：将一段中文句子进行修正，如果句子没有明显语病...   \n",
       "3  [Round 1]\\n\\n问：中文语法纠错任务：将一段中文句子进行修正，如果句子没有明显语病...   \n",
       "4  [Round 1]\\n\\n问：中文语法纠错任务：将一段中文句子进行修正，如果句子没有明显语病...   \n",
       "\n",
       "                                              target  \n",
       "0  按理来说，这种关心应该随着生活水平的提高不断增加才对，现在通过这些单位的执行，反而变成了极大...  \n",
       "1           这些学生往往都具有良好的品质和较高的学习能力，可以通过向优秀榜样学习来提升自我。  \n",
       "2  ”【2】士人们的人生观与哲学观自然也随之转变，而其对山水自然美的认识也从以玄对山水转向以佛对...  \n",
       "3  战略成本会计的切入点是尽可能地降低产品成本，加强企业成本优化管理与控制，其关注点有质量控制、...  \n",
       "4  此外，众多高校在财务管理的内部管理制度方面存在较多纰漏，很多规范过于笼统没有实际可行性，从而...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftrain['context'] = [build_inputs(x,history=his) for x in dftrain['原句']]\n",
    "dftrain['target'] = [x for x in dftrain['改句']]\n",
    "dftrain = dftrain[['context','target']]\n",
    "\n",
    "dftest['context'] = [build_inputs(x,history=his) for x in dftest['原句']]\n",
    "dftest['target'] = [x for x in dftest['改句']]\n",
    "dftest = dftest[['context','target']]\n",
    "\n",
    "dftest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataframe处理成模型能够识别的dataset形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['context', 'target'],\n",
       "    num_rows: 2000\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train = datasets.Dataset.from_pandas(dftrain)\n",
    "ds_val = datasets.Dataset.from_pandas(dftest)\n",
    "ds_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来要进行的是模型的微调：模型作为一个语言模型，其核心在于计算词向量产生的概率。所以我们首先需要构建词向量，之后传入模型并且利用梯度下降的算法来降低模型计算的损失，也就是让模型认识到我们提供的这一批数据就是最可靠的数据。然后在之后使用的过程中，我们就可以通过刚才给出的那个特定的提示词让模型再走一遍类似的推理过程，这样理论上就能够得到比较可靠的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "chatglm2-6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:259\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 259\u001b[0m     response\u001b[39m.\u001b[39mraise_for_status()\n\u001b[1;32m    260\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/chatglm2-6b/resolve/main/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/hub.py:417\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    416\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 417\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    418\u001b[0m         path_or_repo_id,\n\u001b[1;32m    419\u001b[0m         filename,\n\u001b[1;32m    420\u001b[0m         subfolder\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(subfolder) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m subfolder,\n\u001b[1;32m    421\u001b[0m         repo_type\u001b[39m=\u001b[39mrepo_type,\n\u001b[1;32m    422\u001b[0m         revision\u001b[39m=\u001b[39mrevision,\n\u001b[1;32m    423\u001b[0m         cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[1;32m    424\u001b[0m         user_agent\u001b[39m=\u001b[39muser_agent,\n\u001b[1;32m    425\u001b[0m         force_download\u001b[39m=\u001b[39mforce_download,\n\u001b[1;32m    426\u001b[0m         proxies\u001b[39m=\u001b[39mproxies,\n\u001b[1;32m    427\u001b[0m         resume_download\u001b[39m=\u001b[39mresume_download,\n\u001b[1;32m    428\u001b[0m         use_auth_token\u001b[39m=\u001b[39muse_auth_token,\n\u001b[1;32m    429\u001b[0m         local_files_only\u001b[39m=\u001b[39mlocal_files_only,\n\u001b[1;32m    430\u001b[0m     )\n\u001b[1;32m    432\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1195\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     metadata \u001b[39m=\u001b[39m get_hf_file_metadata(\n\u001b[1;32m   1196\u001b[0m         url\u001b[39m=\u001b[39murl,\n\u001b[1;32m   1197\u001b[0m         token\u001b[39m=\u001b[39mtoken,\n\u001b[1;32m   1198\u001b[0m         proxies\u001b[39m=\u001b[39mproxies,\n\u001b[1;32m   1199\u001b[0m         timeout\u001b[39m=\u001b[39metag_timeout,\n\u001b[1;32m   1200\u001b[0m     )\n\u001b[1;32m   1201\u001b[0m \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m http_error:\n\u001b[1;32m   1202\u001b[0m     \u001b[39m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1541\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1532\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1533\u001b[0m     method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mHEAD\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1534\u001b[0m     url\u001b[39m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1539\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[1;32m   1540\u001b[0m )\n\u001b[0;32m-> 1541\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1543\u001b[0m \u001b[39m# Return\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:291\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    283\u001b[0m     message \u001b[39m=\u001b[39m (\n\u001b[1;32m    284\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Client Error.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m make sure you are authenticated.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m     )\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[39melif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-64e08343-47054a5e6b22795709022de7)\n\nRepository Not Found for url: https://huggingface.co/chatglm2-6b/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m max_seq_length \u001b[39m=\u001b[39m \u001b[39m512\u001b[39m\n\u001b[1;32m      6\u001b[0m skip_over_length \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m tokenizer \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39mAutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      9\u001b[0m     model_name, trust_remote_code\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m \u001b[39m#transformer编码器和解码器\u001b[39;00m\n\u001b[1;32m     11\u001b[0m config \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39mAutoConfig\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     12\u001b[0m     model_name, trust_remote_code\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, device_map\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:643\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    640\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    642\u001b[0m \u001b[39m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[0;32m--> 643\u001b[0m tokenizer_config \u001b[39m=\u001b[39m get_tokenizer_config(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    644\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m tokenizer_config:\n\u001b[1;32m    645\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m tokenizer_config[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:487\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[39mLoads the tokenizer configuration from a pretrained model tokenizer configuration.\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[39mtokenizer_config = get_tokenizer_config(\"tokenizer-test\")\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[39m```\"\"\"\u001b[39;00m\n\u001b[1;32m    486\u001b[0m commit_hash \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 487\u001b[0m resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    488\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m    489\u001b[0m     TOKENIZER_CONFIG_FILE,\n\u001b[1;32m    490\u001b[0m     cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[1;32m    491\u001b[0m     force_download\u001b[39m=\u001b[39mforce_download,\n\u001b[1;32m    492\u001b[0m     resume_download\u001b[39m=\u001b[39mresume_download,\n\u001b[1;32m    493\u001b[0m     proxies\u001b[39m=\u001b[39mproxies,\n\u001b[1;32m    494\u001b[0m     use_auth_token\u001b[39m=\u001b[39muse_auth_token,\n\u001b[1;32m    495\u001b[0m     revision\u001b[39m=\u001b[39mrevision,\n\u001b[1;32m    496\u001b[0m     local_files_only\u001b[39m=\u001b[39mlocal_files_only,\n\u001b[1;32m    497\u001b[0m     subfolder\u001b[39m=\u001b[39msubfolder,\n\u001b[1;32m    498\u001b[0m     _raise_exceptions_for_missing_entries\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    499\u001b[0m     _raise_exceptions_for_connection_errors\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    500\u001b[0m     _commit_hash\u001b[39m=\u001b[39mcommit_hash,\n\u001b[1;32m    501\u001b[0m )\n\u001b[1;32m    502\u001b[0m \u001b[39mif\u001b[39;00m resolved_config_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    503\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/hub.py:433\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    417\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    418\u001b[0m         path_or_repo_id,\n\u001b[1;32m    419\u001b[0m         filename,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    429\u001b[0m         local_files_only\u001b[39m=\u001b[39mlocal_files_only,\n\u001b[1;32m    430\u001b[0m     )\n\u001b[1;32m    432\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[0;32m--> 433\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    434\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    438\u001b[0m     )\n\u001b[1;32m    439\u001b[0m \u001b[39mexcept\u001b[39;00m RevisionNotFoundError:\n\u001b[1;32m    440\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    441\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mrevision\u001b[39m}\u001b[39;00m\u001b[39m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    442\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfor this model name. Check the model page at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    443\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m for available revisions.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    444\u001b[0m     )\n",
      "\u001b[0;31mOSError\u001b[0m: chatglm2-6b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`."
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import transformers\n",
    "\n",
    "model_name = \"chatglm2-6b\"\n",
    "max_seq_length = 512\n",
    "skip_over_length = True\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name, trust_remote_code=True)\n",
    "#transformer编码器和解码器\n",
    "config = transformers.AutoConfig.from_pretrained(\n",
    "    model_name, trust_remote_code=True, device_map='auto')\n",
    "#用于根据输入的模型名称自动设置模型参数，以便于进行模型的初始化和实例化。\n",
    "def preprocess(example):#对一条数据进行编码。\n",
    "    context = example[\"context\"]\n",
    "    target = example[\"target\"]\n",
    "    \n",
    "    context_ids = tokenizer.encode(\n",
    "            context, \n",
    "            max_length=max_seq_length,\n",
    "            truncation=True)\n",
    "    \n",
    "    target_ids = tokenizer.encode(\n",
    "        target,\n",
    "        max_length=max_seq_length,\n",
    "        truncation=True,\n",
    "        add_special_tokens=False)\n",
    "    \n",
    "    input_ids = context_ids + target_ids + [config.eos_token_id]#添加一个结束符\n",
    "    \n",
    "    return {\"input_ids\": input_ids, \"context_len\": len(context_ids),'target_len':len(target_ids)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ds_train_token \u001b[39m=\u001b[39m ds_train\u001b[39m.\u001b[39mmap(preprocess)\u001b[39m.\u001b[39mselect_columns([\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcontext_len\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mtarget_len\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      2\u001b[0m \u001b[39m#将函数preprocess迭代地作用与ds_train中的每一个元素，结果返回给ds_train_token\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mif\u001b[39;00m skip_over_length:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocess' is not defined"
     ]
    }
   ],
   "source": [
    "ds_train_token = ds_train.map(preprocess).select_columns(['input_ids', 'context_len','target_len'])\n",
    "#将函数preprocess迭代地作用与ds_train中的每一个元素，结果返回给ds_train_token\n",
    "if skip_over_length:\n",
    "    ds_train_token = ds_train_token.filter(\n",
    "        lambda example: example[\"context_len\"]<max_seq_length and example[\"target_len\"]<max_seq_length)\n",
    "        #略过编码长度过大的序列，这里显得有些重复但是我不敢改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_val_token = ds_val.map(preprocess).select_columns(['input_ids', 'context_len','target_len'])\n",
    "if skip_over_length:\n",
    "    ds_val_token = ds_val_token.filter(\n",
    "        lambda example: example[\"context_len\"]<max_seq_length and example[\"target_len\"]<max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建管道"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(features: list):\n",
    "    len_ids = [len(feature[\"input_ids\"]) for feature in features]\n",
    "    longest = max(len_ids) #之后按照batch中最长的input_ids进行padding\n",
    "    \n",
    "    input_ids = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for length, feature in sorted(zip(len_ids, features), key=lambda x: -x[0]):\n",
    "        ids = feature[\"input_ids\"]\n",
    "        context_len = feature[\"context_len\"]\n",
    "        \n",
    "        labels = (\n",
    "            [-100] * (context_len - 1) + ids[(context_len - 1) :] + [-100] * (longest - length)\n",
    "        ) #-100标志位后面会在计算loss时会被忽略不贡献损失，我们集中优化target部分生成的loss\n",
    "        \n",
    "        ids = ids + [tokenizer.pad_token_id] * (longest - length)\n",
    "        \n",
    "        input_ids.append(torch.LongTensor(ids))\n",
    "        labels_list.append(torch.LongTensor(labels))\n",
    "        \n",
    "        \n",
    "    input_ids = torch.stack(input_ids)\n",
    "    labels = torch.stack(labels_list)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "dl_train = torch.utils.data.DataLoader(ds_train_token,num_workers=2,batch_size=4,\n",
    "                                       pin_memory=True,shuffle=True,\n",
    "                                       collate_fn = data_collator)\n",
    "dl_val = torch.utils.data.DataLoader(ds_val_token,num_workers=2,batch_size=4,\n",
    "                                    pin_memory=True,shuffle=True,\n",
    "                                     collate_fn = data_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dl_train:\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train.size = 300 #每300个step视作一个epoch，做一次验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, TrainingArguments, AutoConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "model = AutoModel.from_pretrained(\"chatglm2-6b\",\n",
    "                                  load_in_8bit=False, \n",
    "                                  trust_remote_code=True, \n",
    "                                  device_map='auto')\n",
    "\n",
    "model.supports_gradient_checkpointing = True  #节约cuda\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "#model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32, lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.is_parallelizable = True\n",
    "model.model_parallel = True\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchkeras import KerasModel \n",
    "from accelerate import Accelerator \n",
    "\n",
    "class StepRunner:\n",
    "    def __init__(self, net, loss_fn, accelerator=None, stage = \"train\", metrics_dict = None, \n",
    "                 optimizer = None, lr_scheduler = None\n",
    "                 ):\n",
    "        self.net,self.loss_fn,self.metrics_dict,self.stage = net,loss_fn,metrics_dict,stage\n",
    "        self.optimizer,self.lr_scheduler = optimizer,lr_scheduler\n",
    "        self.accelerator = accelerator if accelerator is not None else Accelerator() \n",
    "        if self.stage=='train':\n",
    "            self.net.train() \n",
    "        else:\n",
    "            self.net.eval()\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \n",
    "        #loss\n",
    "        with self.accelerator.autocast():\n",
    "            loss = self.net(input_ids=batch[\"input_ids\"],labels=batch[\"labels\"]).loss\n",
    "\n",
    "        #backward()\n",
    "        if self.optimizer is not None and self.stage==\"train\":\n",
    "            self.accelerator.backward(loss)\n",
    "            if self.accelerator.sync_gradients:\n",
    "                self.accelerator.clip_grad_norm_(self.net.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "            if self.lr_scheduler is not None:\n",
    "                self.lr_scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "        all_loss = self.accelerator.gather(loss).sum()\n",
    "        \n",
    "        #losses (or plain metrics that can be averaged)\n",
    "        step_losses = {self.stage+\"_loss\":all_loss.item()}\n",
    "        \n",
    "        #metrics (stateful metrics)\n",
    "        step_metrics = {}\n",
    "        \n",
    "        if self.stage==\"train\":\n",
    "            if self.optimizer is not None:\n",
    "                step_metrics['lr'] = self.optimizer.state_dict()['param_groups'][0]['lr']\n",
    "            else:\n",
    "                step_metrics['lr'] = 0.0\n",
    "        return step_losses,step_metrics\n",
    "    \n",
    "KerasModel.StepRunner = StepRunner \n",
    "\n",
    "\n",
    "#仅仅保存lora可训练参数\n",
    "def save_ckpt(self, ckpt_path='checkpoint.pt', accelerator = None):\n",
    "    unwrap_net = accelerator.unwrap_model(self.net)\n",
    "    unwrap_net.save_pretrained(ckpt_path)\n",
    "    \n",
    "def load_ckpt(self, ckpt_path='checkpoint.pt'):\n",
    "    self.net = self.net.from_pretrained(self.net,ckpt_path)\n",
    "    self.from_scratch = False\n",
    "    \n",
    "KerasModel.save_ckpt = save_ckpt \n",
    "KerasModel.load_ckpt = load_ckpt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = KerasModel(model,loss_fn = None,\n",
    "        optimizer=torch.optim.AdamW(model.parameters(),lr=2e-6))\n",
    "ckpt_path = 'waimai_chatglm4'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model.fit(train_data = dl_train,\n",
    "                val_data = dl_val,\n",
    "                epochs=100,patience=5,\n",
    "                monitor='val_loss',mode='min',\n",
    "                ckpt_path = ckpt_path,\n",
    "                mixed_precision='fp16'\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel \n",
    "model = AutoModel.from_pretrained(\"chatglm2-6b\",\n",
    "                                  load_in_8bit=False, \n",
    "                                  trust_remote_code=True, \n",
    "                                  device_map='auto')\n",
    "model = PeftModel.from_pretrained(model,ckpt_path)\n",
    "model = model.merge_and_unload() #合并lora权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    response, history = model.chat(tokenizer, f\"{text} -> \", history=his,\n",
    "    temperature=0.01)\n",
    "    return response \n",
    "\n",
    "predict(\"现在，我的生活非常充实，非常满意。\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest = pd.read_parquet('data/dftest.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = ['' for x in dftest['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "for i in tqdm(range(len(dftest))):\n",
    "    text = dftest['text'].loc[i]\n",
    "    preds[i] = predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest['pred'] = preds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest.pivot_table(index='tag',columns = 'pred',values='text',aggfunc='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = len(dftest.query('tag==pred'))/len(dftest)\n",
    "print('acc=',acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用模型&调整模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m运行具有“/bin/python3.10”的单元格需要ipykernel包。\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3.10 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def predict(text,temperature=0.8):\n",
    "    response, history = model.chat(tokenizer, f\"{text} -> \", history=his,\n",
    "    temperature=temperature)\n",
    "    return response \n",
    "\n",
    "for i in range(10):\n",
    "    print(predict(\"现在，我的生活非常充实，非常满意。\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"chatglm2-6b-waimai\", max_shard_size='1GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"chatglm2-6b-CGEC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls chatglm2-6b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp  chatglm2-6b/*.py chatglm2-6b-CGEC/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls chatglm2-6b-CGEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  AutoModel,AutoTokenizer\n",
    "model_name = \"chatglm2-6b-waimai\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name,\n",
    "        trust_remote_code=True).half().cuda()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
